supabase:
  tables:
    organizations: {pk: id}
    org_members: {pk: [org_id, user_id], cols: [role, created_at]}
    profiles: {pk: user_id, cols: [full_name, locale, professional_type, bar_number?, court_id?, verified:boolean]}
    org_policies:
      pk: id
      cols: [org_id, key, value:jsonb, created_at, updated_at]
      unique: [[org_id, key]]
    jurisdiction_entitlements:
      pk: [org_id, juris_code]
      cols: [can_read:boolean, can_write:boolean, created_at, updated_at]

    jurisdictions: {pk: id, cols: [code, name, eu:boolean, ohada:boolean]}
    authority_domains: {pk: id, unique: [jurisdiction_code, host]}

    sources:
      pk: id
      cols: [org_id, jurisdiction_code, source_type, title, publisher, source_url,
             binding_lang, consolidated:boolean, adopted_date:date, effective_date:date,
             eli_celex?, ecli?, capture_sha256, access_rights, notes]
      idx: [jurisdiction_code, source_type, effective_date]

    documents:
      pk: id
      cols: [org_id, source_id?, name, storage_path, openai_file_id?, mime_type, bytes]

    document_chunks:
      pk: id
      cols: [org_id, document_id, jurisdiction_code, content, embedding(vector-1536), seq:int, article_or_section?]
      idx: ["hnsw(embedding)", jurisdiction_code]

    agent_runs:
      pk: id
      cols: [org_id, user_id, question, jurisdiction_json, model, started_at, finished_at,
             risk_level, hitl_required:boolean, irac:jsonb, status]

    tool_invocations: {pk: id, cols: [run_id, tool_name, args:jsonb, output:text]}
    run_citations: {pk: id, cols: [run_id, title, publisher, date, url, domain_ok:boolean]}
    hitl_queue: {pk: id, cols: [run_id, org_id, reason, status, reviewer_id, created_at, updated_at]}
    hitl_reviewer_edits: {pk: id, cols: [hitl_id, run_id, org_id, reviewer_id, action, comment, previous_payload, revised_payload, created_at]}
    compliance_assessments:
      pk: id
      cols: [org_id, run_id, fria_required:boolean, fria_reasons:text[], cepej_passed:boolean, cepej_violations:text[], created_at]
      unique: [[run_id]]

    transparency_reports:
      pk: id
      cols: [org_id, generated_by, period_start:date, period_end:date, generated_at, distribution_status, metrics:jsonb, cepej_summary:jsonb]
      idx: [[org_id, period_end desc]]

    audit_events:
      pk: id
      cols: [org_id, actor_user_id?, kind, object, before_state:jsonb?, after_state:jsonb?, metadata:jsonb?, created_at]
    consent_events:
      pk: id
      cols: [org_id, user_id, consent_type, version, created_at]
    invitations:
      pk: token
      cols: [org_id, email, role, expires_at, accepted_by?, accepted_at?, created_at]
    billing_accounts:
      pk: org_id
      cols: [plan, seats:int, metering:jsonb, created_at, updated_at]

    eval_cases: {pk: id, cols: [org_id, name, prompt, expected_contains:text[]]}
    eval_results: {pk: id, cols: [case_id, run_id?, pass:boolean, notes]}

    # Learning & ops
    agent_task_queue: {pk: id, cols: [type, org_id, payload:jsonb, priority:int, status, timestamps]}
    agent_learning_jobs: {pk: id, cols: [type, payload:jsonb, status, timestamps]}
    agent_synonyms: {pk: id, cols: [jurisdiction, term, expansions:text[]]}
    agent_policy_versions: {pk: id, cols: [name, activated_at, notes]}
    tool_telemetry: {pk: id, cols: [run_id, tool_name, latency_ms:int, success:boolean, error_code?]}

    # Case reliability scoring
    case_scores:
      pk: id
      cols: [source_id, juris_code, score_overall:int, axes:jsonb, version, computed_at, model_ref, notes]
      idx: [juris_code, score_overall]

    case_treatments:
      pk: id
      cols: [case_id, citing_case_id, treatment, court_rank, weight:float, decided_at:date]

    case_statute_links:
      pk: id
      cols: [case_id, statute_url, article, alignment_score:int, rationale_json:jsonb]

    risk_register:
      pk: id
      cols: [juris_code, court_id?, period_from?:date, period_to?:date, risk_flag, note]

    score_overrides:
      pk: id
      cols: [case_id, reviewer_id, new_score:int, reason, created_at]

    red_team_findings:
      pk: id
      cols: [org_id, scenario_key, severity, expected_outcome, observed_outcome, passed:boolean,
             summary, detail:jsonb?, mitigations?, status, detected_at, resolved_at?, resolved_by?,
             created_by, updated_at]
      idx: [[org_id], [scenario_key]]

    performance_snapshots:
      pk: id
      cols: [org_id, window_label, collected_at, total_runs, avg_latency_ms?, p95_latency_ms?,
             allowlisted_ratio?, hitl_median_minutes?, citation_precision?, temporal_validity?,
             binding_warnings?, notes?, recorded_by?, metadata:jsonb?]

    slo_snapshots:
      pk: id
      cols: [org_id, captured_at, api_uptime_percent, hitl_response_p95_seconds,
             retrieval_latency_p95_seconds, citation_precision_p95?, notes?, created_by, created_at]

    regulator_dispatches:
      pk: id
      cols: [org_id, report_type, period_start, period_end, payload_url?, status,
             metadata:jsonb?, created_by, created_at, dispatched_at?]

    go_no_go_evidence:
      pk: id
      cols: [org_id, section, criterion, status, evidence_url?, notes:jsonb?, recorded_by, recorded_at]
      idx: [[org_id, section]]

    go_no_go_signoffs:
      pk: id
      cols: [org_id, release_tag, decision, decided_by, decided_at, notes?, evidence_total:int]
      unique: [[org_id, release_tag]]

    fria_artifacts:
      pk: id
      cols: [org_id, release_tag?, title, evidence_url?, storage_path?, hash_sha256?,
             validated:boolean, submitted_by, submitted_at, notes:jsonb?]
      idx: [[org_id], [org_id, release_tag]]

  rls:
    model: "per-tenant (public.is_org_member(org_id)) on all org-scoped tables"
    storage: "bucket paths prefix with org_id/"
    secrets: "no service keys client-side; server-only"

  rpc:
    match_chunks(query_embedding: vector-1536, org_id: uuid, k:int=8, juris?:text) -> top-k
    health_checks: ["citations_link_health", "allowlist_violations"]
  views:
    cepej_metrics: "org-level CEPEJ/FRIA summary (assessed_runs, pass_rate, violation counts)"
    cepej_violation_breakdown: "per-org violation histogram (transparency, quality_security, fundamental_rights_screening, user_control)"
    org_retrieval_metrics: "per-org hybrid retrieval summary (runs_total, snippet mix, allowlist ratio, translation flags, last run)"
    org_retrieval_origin_metrics: "snippet counts and similarity/weight averages grouped by origin (local vs file_search)"
    org_retrieval_host_metrics: "top citation hosts with allowlist coverage, translation warnings, and last cited timestamps"

agent_tools:
  hosted:
    - web_search
    - file_search  # attached to OPENAI_VECTOR_STORE_AUTHORITIES_ID
  functions:
    - name: routeJurisdiction
      in: {question:string}
      out: {country:string, eu:boolean, ohada:boolean, confidence:float}
    - name: lookupCodeArticle
      in: {jurisdiction:string, code:string, article:string}
      out: {url:string, title:string, publisher:string, consolidation:boolean, effective_date?:date}
    - name: ohadaUniformAct
      in: {topic:string, subtopic?:string}
      out: {acte:string, articles:string[], dates:object, url:string}
    - name: deadlineCalculator
      in: {jurisdiction:string, procedure_type:string, start_date:date, service_method?:string}
      out: {deadline_date:date, computation_notes:string[], risk_flags:string[]}
    - name: limitationCheck
      in: {jurisdiction:string, claim_type:string, trigger_date:date, tolling?:string[]}
      out: {limit_years:int, deadline_date?:date, notes:string[]}
    - name: redlineContract
      in: {doc_id:string, jurisdiction:string}
      out: {diff:jsonb, rationale:string[], citations:string[]}
    - name: generateTemplate
      in: {jurisdiction:string, matter_type:string}
      out: {sections:string[], fill_ins:string[]}
    - name: validateCitation
      in: {url:string}
      out: {allowlisted:boolean, domain:string, reason?:string}
    - name: checkBindingLanguage
      in: {url:string, juris_code:string}
      out: {binding_lang:string, translation_notice?:string}
    - name: snapshotAuthority
      in: {url:string}
      out: {storage_path:string, hash:string, doc_id:string}
    - name: computeCaseScore
      in: {source_id:string}
      out: {score_overall:int, axes:jsonb}
    - name: buildTreatmentGraph
      in: {since?:date}
      out: {edges:int, updated_cases:int}

guardrails:
  outputs:
    - citations_allowlist
    - binding_language_guardrail
    - structured_output_irac  # hard fail → retry once → HITL
    - sensitive_topic_hitl    # penal/elections/national security
  policies:
    statute_first: true
    maghreb_language_banner: true
    ohada_preemption_priority: true
    france_judge_analytics_block: true

rbac:
  roles: [owner, admin, member, reviewer, viewer, compliance_officer, auditor]
  permissions:
    research.run:        [member, reviewer, admin, owner]
    drafting.edit:       [member, reviewer, admin, owner]
    hitl.review:         [reviewer, admin, owner]
    corpus.manage:       [admin, owner]
    policies.manage:     [admin, owner, compliance_officer]
    billing.manage:      [owner]
    audit.read:          [auditor, compliance_officer, admin, owner]
    allowlist.toggle:    [admin, owner]
    residency.change:    [owner]
    sso_scim.manage:     [admin, owner]
    data.export_delete:  [owner]

abac:
  tables:
    org_policies: "policy flags such as confidential_mode, fr_judge_analytics_block, mfa_required, ip_allowlist"
    jurisdiction_entitlements: "jurisdictional feature gates controlling OHADA/EU overlays and Maghreb/Rwanda toggles"
  request_context: [org_id, user_id, roles, jurisdiction_entitlements, org_policies]

retrieval:
  tiers:
    T1: "statutes/regulations/official gazettes (highest weight)"
    T2: "apex & high courts (official sites)"
    T3: "trial courts"
    T4: "secondary sources (LII/commentary)"
  weights:
    T1: 1.00
    T2: 0.85
    T3: 0.45
    T4: 0.20
  penalties:
    negative_treatment: -0.50
    pending_appeal: -0.30
    political_risk_flag: -0.40
  hard_blocks:
    overruled: true
    vacated: true
  bilingual_rules:
    canada_equal_force: true
  rwanda_languages: ["fr","en","rw"]

acceptance_thresholds:
  citations_allowlisted_p95: 0.95
  temporal_validity_p95: 0.95
  maghreb_binding_banner_coverage: 1.00
  hitl_recall_high_risk: 0.98

Next 5 Actions (immediately actionable)

- Freeze the allowlist & ELI/ECLI mappers and backfill two pilot jurisdictions (FR, OHADA) with Akoma-Ntoso anchors.
- Enable case-scoring pipeline + treatment graph; surface score badges in the Citations panel.
- Turn on guardrails (citations_allowlist, binding_language, sensitive_topic_hitl) and verify refusal paths.
- Run the Go/No-Go checklist end-to-end with a 50-doc ingestion per jurisdiction; capture metrics dashboards.
- Ship the France judge-analytics blocker and publish the EU AI Act FRIA + CEPEJ compliance note on your Trust page.
