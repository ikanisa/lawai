# Function Calling Integration Guide

This guide explains how to expose application capabilities to OpenAI models through function and custom tool calls. It walks through the request/response cycle, highlights schema design guidance, and captures operational considerations for running tools in production.

## Key Concepts
- **Tools** are capabilities that your application makes available to the model. Each tool advertises how it should be invoked and what it returns.
- **Tool Calls** are the model's requests to run a tool. A single response can contain zero, one, or many tool calls.
- **Tool Call Outputs** are the values your application returns to the model after executing the requested logic.
- **Function Tools** are JSON-schema described tools. The schema defines the function name, the arguments the model must supply, and whether strict validation is enforced.
- **Custom Tools** accept unstructured string inputs. They are useful when the payload is easier to express in free text or when you want to layer a custom grammar over the model's output.

## The Tool Calling Flow
1. **Request** – Send a prompt to the model along with the list of tools that may be invoked.
2. **Model Decision** – Inspect the response for any `function_call` or `custom_tool_call` items.
3. **Execute** – Run the referenced tool in your application, using the JSON arguments (functions) or raw string payload (custom tools).
4. **Return Output** – Append a `function_call_output` entry to the conversation, keyed by `call_id`, containing the serialized result.
5. **Follow-up** – Re-issue the conversation to the model so it can incorporate the tool outputs, or continue chaining further tool calls.

```python
from openai import OpenAI
import json

client = OpenAI()

tools = [
    {
        "type": "function",
        "name": "get_horoscope",
        "description": "Get today's horoscope for an astrological sign.",
        "parameters": {
            "type": "object",
            "properties": {
                "sign": {
                    "type": "string",
                    "description": "Astrological sign, e.g. Taurus or Aquarius",
                }
            },
            "required": ["sign"],
        },
    }
]

input_list = [
    {"role": "user", "content": "What is my horoscope? I am an Aquarius."}
]

response = client.responses.create(
    model="gpt-5",
    tools=tools,
    input=input_list,
)

input_list += response.output

for item in response.output:
    if item.type == "function_call" and item.name == "get_horoscope":
        horoscope = get_horoscope(json.loads(item.arguments))
        input_list.append({
            "type": "function_call_output",
            "call_id": item.call_id,
            "output": json.dumps({"horoscope": horoscope}),
        })

follow_up = client.responses.create(
    model="gpt-5",
    instructions="Respond only with a horoscope generated by a tool.",
    tools=tools,
    input=input_list,
)
```

> **Note:** Reasoning models (for example GPT-5 or o4-mini) may emit hidden reasoning items alongside tool calls. Include those reasoning entries unchanged when you append tool outputs back into the conversation history.

## Designing Function Schemas
Function definitions are supplied in the `tools` array and must include:
- `type`: Always `function` for schema-driven tools.
- `name`: A concise, descriptive identifier.
- `description`: Guidance on what the function does and when to use it.
- `parameters`: A JSON schema describing the inputs. You can nest objects, use enums, and mark optional properties by including `null` in the `type` array.
- `strict`: When `true`, the model must produce arguments that match the schema exactly (requires `additionalProperties: false` and all properties listed in `required`).

```json
{
  "type": "function",
  "name": "get_weather",
  "description": "Retrieves current weather for the given location.",
  "parameters": {
    "type": "object",
    "properties": {
      "location": {
        "type": "string",
        "description": "City and country, e.g. Bogotá, Colombia"
      },
      "units": {
        "type": "string",
        "enum": ["celsius", "fahrenheit"],
        "description": "Unit for the returned temperature."
      }
    },
    "required": ["location", "units"],
    "additionalProperties": false
  },
  "strict": true
}
```

### Schema Best Practices
- Provide explicit names, descriptions, and formats for each argument.
- Move deterministic data out of function arguments—pass it from your application code when possible.
- Combine operations that are always executed together into a single function to reduce call churn.
- Keep the total number of exposed tools manageable (ideally fewer than twenty).
- Prototype schemas in the Playground and iterate on real model outputs before shipping.

## Handling Tool Calls
The response `output` array can include multiple tool calls. Iterate through each entry, decode the arguments, and route the call to your application logic. Return a string payload that captures success, failure, or structured JSON data.

```python
for tool_call in response.output:
    if tool_call.type != "function_call":
        continue

    args = json.loads(tool_call.arguments)
    result = dispatch(tool_call.name, args)

    input_messages.append({
        "type": "function_call_output",
        "call_id": tool_call.call_id,
        "output": json.dumps(result),
    })
```

If a tool has no return value (e.g. `send_email`), return a status string such as `"success"` so the model can acknowledge completion.

## Operational Controls
- **Tool Choice:** Configure `tool_choice` to force or limit tool usage. Options include `auto`, `required`, specific functions, or `allowed_tools` subsets.
- **Parallel Calls:** Set `parallel_tool_calls` to `false` to ensure the model requests at most one tool at a time.
- **Strict Mode:** Enforce schema compliance by setting `strict: true` and defining `additionalProperties: false`. Note that optional fields must include `null` in the `type` and still appear in `required`.
- **Token Usage:** Function definitions are injected into the system message. Optimize descriptions and limit the number of functions to stay within context and budget constraints.

## Streaming Tool Calls
Enable streaming with `stream=True` to receive incremental argument updates while the model fills out a function call. Monitor the events:
- `response.output_item.added` – signals a new tool call with empty arguments.
- `response.function_call_arguments.delta` – incremental JSON fragments.
- `response.function_call_arguments.done` – final JSON payload for the call.

Aggregate deltas per `output_index` until the `done` event arrives, then execute the tool and return results as usual.

## Custom Tools and Grammars
Custom tools (`type: "custom"`) accept free-form string inputs. Use them when JSON wrappers add unnecessary friction or when the payload must conform to a custom grammar.

```python
response = client.responses.create(
    model="gpt-5",
    input="Use the code_exec tool to print hello world to the console.",
    tools=[
        {
            "type": "custom",
            "name": "code_exec",
            "description": "Executes arbitrary Python code.",
        }
    ],
)
```

To constrain inputs, attach a grammar:
- **Lark grammar:** Provide a `format` block with `syntax: "lark"` and a grammar definition. Keep rules simple—avoid lookarounds, lazy quantifiers, and complex `%ignore` usage.
- **Regex grammar:** Supply a single-line Rust-regex-compatible pattern with named capture groups if needed. Newlines must be expressed with `\n`.

## Troubleshooting & Tips
- Simplify grammars if the API rejects them as too complex.
- Watch for greedy lexer behavior when designing Lark grammars; encapsulate free text in a single terminal.
- Tighten grammars and prompts if outputs drift semantically despite being syntactically valid.
- Cache schemas where possible to avoid repeated processing overhead, especially with fine-tuned models.
- When using reasoning models, ensure that reasoning items are forwarded unchanged alongside tool call outputs.

By following these practices, you can reliably expose application behaviors to OpenAI models and orchestrate complex, multi-step workflows through function and custom tool calls.
